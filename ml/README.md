# ИИ-агент с RAG Fusion и передачей запроса оператору поддержки

Этот проект реализует чат-бот поддержки с использованием LangChain с технологией RAG Fusion для поиска информации и возможностью передачи сложных запросов операторам-людям.

## Компоненты

1. **База знаний (knowledge_base.py):**
   - Обрабатывает загрузку, обработку и векторизацию документов
   - Предоставляет функциональность векторного поиска с использованием FAISS
   - Поддерживает различные форматы документов (текст, JSON, PDF)

2. **ИИ-агент (ai_agent.py):**
   - Реализует стратегию поиска RAG Fusion для улучшенных результатов
   - Использует семантическое индексирование для повышения точности ответов
   - Применяет гибридный поиск с динамическим взвешиванием результатов
   - Поддерживает историю разговора
   - Включает определение необходимости передачи запроса оператору
   - Построен на LangChain и Ollama

3. **API сервер (api_server.py):**
   - Реализация FastAPI для обслуживания ИИ-агента
   - Эндпоинты для запросов к ИИ и загрузки файлов в базу знаний
   - Поддерживает управление разговорами и фоновую обработку данных
   - Автоматическое обновление базы знаний при загрузке новых PDF-файлов

## Технологии и особенности

### RAG Fusion
Улучшенная версия классической архитектуры RAG (Retrieval-Augmented Generation), которая:
- Генерирует несколько вариантов поисковых запросов из исходного вопроса пользователя
- Обрабатывает результаты с использованием алгоритма Reciprocal Rank Fusion
- Повышает точность и релевантность ответов

### Семантическое индексирование
Система использует семантическое индексирование для повышения качества ответов:
- **Семантическое разбиение**: Разделение текста на логически связанные части с учетом смысловой близости
- **Интеллектуальная группировка**: Объединение семантически близких предложений в единые блоки
- **Семантический поиск**: Ранжирование результатов по семантической близости к запросу пользователя
- **Повышение точности контекста**: Предоставление модели только семантически релевантных частей документов

### Гибридный поиск с динамическим взвешиванием
Система объединяет несколько поисковых стратегий для максимальной точности результатов:
- **Векторный поиск**: Поиск документов с использованием семантических эмбеддингов
- **BM25 поиск**: Алгоритм ранжирования на основе частоты слов, улучшенная версия TF-IDF
- **Динамическое взвешивание**: Автоматическая настройка весов различных поисковых стратегий в зависимости от запроса
- **TF-IDF оценка релевантности**: Дополнительная оценка релевантности найденных документов с помощью TF-IDF
- **Комбинирование результатов**: Интеллектуальное объединение результатов разных поисковых методов

## Требования

- Python 3.9+
- [Ollama](https://ollama.ai/) установленный локально
- Модель Deepseek R1 7B, загруженная в Ollama

## Установка

1. **Установка зависимостей:**

```bash
pip install -r requirements.txt
```

2. **Установка и запуск Ollama:**

Следуйте инструкциям на [https://ollama.ai/](https://ollama.ai/) для установки Ollama.

3. **Загрузка модели Deepseek R1 7B:**

```bash
ollama pull deepseek-r1:7b
```

4. **Подготовка базы знаний:**

Создайте директорию для документов базы знаний:

```bash
mkdir -p knowledge
```

Добавьте текстовые документы, PDF-файлы, FAQ и другой контент для базы знаний в директорию `knowledge`.

5. **Создание векторного хранилища:**

```python
from knowledge_base import KnowledgeBase

# Инициализация базы знаний
kb = KnowledgeBase()

# Загрузка и обработка документов
documents = kb.load_and_process_documents(
    directory="./knowledge",
    files=["./faq.txt"],  # Добавьте конкретные файлы при необходимости
    json_files={"./data.json": ".content"}  # Добавьте JSON данные с экстракторами контента
)

# Создание векторного хранилища
kb.create_vector_store(documents, persist_directory="./vector_store")
```

## Использование

### Режим командной строки

Запустите ИИ-агента в режиме командной строки:

```bash
python ai_agent.py
```

Взаимодействуйте с агентом, вводя сообщения. Агент будет:
- Искать в базе знаний с использованием RAG Fusion и гибридного поиска
- Применять семантическое индексирование для улучшения результатов
- Предоставлять ответы на основе релевантной информации
- Предлагать передачу запроса оператору, когда не может уверенно ответить

Специальные команды:
- Введите `exit` для выхода
- Введите `reset` для очистки истории разговора

### API сервер

Запустите API сервер:

```bash
python api_server.py
```

Это запустит FastAPI сервер на порту 8000 со следующими эндпоинтами:

- **POST /query**: Запрос к ИИ-агенту
  ```json
  {
    "query": "Как сбросить пароль?",
    "conversation_id": "опциональный-id-разговора",
    "reset_conversation": false
  }
  ```

- **POST /upload**: Загрузка новых файлов в базу знаний
  ```bash
  curl -X POST -F "files=@document.pdf" -F "rebuild_index=true" http://localhost:8000/upload
  ```

- **GET /rebuild_status**: Проверка статуса обновления базы знаний

- **POST /rebuild**: Ручной запуск обновления базы знаний

## Как работает RAG Fusion

RAG Fusion улучшает поиск следующим образом:
1. Генерирует несколько поисковых запросов на основе вопроса пользователя
2. Извлекает документы для каждого запроса
3. Объединяет результаты с помощью алгоритма обратного рангового слияния
4. Ранжирует документы для нахождения наиболее релевантной информации

Этот подход обеспечивает более полные и точные результаты по сравнению с поиском по одному запросу.

## Передача оператору

Агент определяет, когда он не может уверенно ответить на вопрос, и предлагает передать запрос оператору-человеку. Это срабатывает при:
- Отсутствии релевантной информации в базе знаний
- Сложных запросах за пределами возможностей агента
- Явных запросах на помощь от человека

В производственной среде вы можете интегрировать это с системой тикетов поддержки или платформой чата.

# RAG-агент с использованием RAG Fusion и графа знаний

Система поддержки пользователей на основе RAG Fusion с графом знаний, семантическим разбиением и гибридным поиском. Система использует локальную модель через Ollama.

## Основные особенности

- **RAG Fusion** - многозапросная стратегия, объединяющая результаты множества запросов
- **Граф знаний** - семантический граф отношений между сущностями, извлеченными из текста
- **Гибридный поиск** - комбинирует векторный и текстовый (BM25) поиск для лучших результатов
- **Семантическое разбиение** - разделение текста по семантическим блокам для лучшего поиска
- **Диалоговый режим** - поддержка контекста и историй взаимодействия
- **API-интерфейс** - простой REST API для интеграции
- **Кастомная база знаний** - возможность загрузки собственных документов

## Требования

- Python 3.8+
- [Ollama](https://ollama.ai/) с установленной моделью
- Neo4j (опционально, для графа знаний)

## Установка

1. Клонировать репозиторий:
```bash
git clone https://github.com/user/rag-fusion.git
cd rag-fusion
```

2. Установить зависимости:
```bash
pip install -r requirements.txt
```

3. Установить Ollama и загрузить модель:
```bash
ollama pull deepseek-r1:7b
```

4. (Опционально) Установить Neo4j для графа знаний:
```bash
# Для Docker:
docker run -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/password neo4j:latest
```

## Использование

### Создание базы знаний

1. Поместите ваши документы (TXT и PDF) в директорию `knowledge/`
2. Запустите скрипт создания базы знаний:
```bash
python create_knowledge_base.py
```

### Запуск агента в консоли

```bash
python ai_agent.py
```

### Запуск API-сервера

```bash
python api_server.py
```

API будет доступен по адресу http://localhost:8000

### Настройка графа знаний

Для использования графа знаний нужно установить переменные окружения:

```bash
export NEO4J_URI=bolt://localhost:7687
export NEO4J_USERNAME=neo4j
export NEO4J_PASSWORD=password
export USE_KNOWLEDGE_GRAPH=true

python api_server.py
```

## API Endpoints

### Получение ответа на запрос

```
POST /query
```

Параметры запроса:
```json
{
  "query": "Сколько стоит тариф Стандарт?",
  "conversation_id": "user123", 
  "reset_conversation": false
}
```

Пример ответа:
```json
{
  "response": "Тариф Стандарт стоит 1500 рублей в месяц...",
  "human_handoff": false,
  "conversation_id": "user123",
  "source_documents": [...],
  "used_files": ["tariffs.txt", "pricing.pdf"]
}
```

### Загрузка новых файлов

```
POST /upload
```

Мультичастная форма с полями:
- `files`: Файлы для загрузки (PDF или TXT)
- `rebuild_index`: Булево значение (true/false)

### Проверка статуса обновления

```
GET /rebuild_status
```

## Архитектура

### Компоненты системы

1. **Модуль базы знаний** (`knowledge_base.py`)
   - Загрузка и обработка документов
   - Создание и обновление векторного хранилища

2. **Модуль графа знаний**
   - Извлечение сущностей и отношений из текста
   - Построение графа знаний в Neo4j
   - Использование графа для обогащения результатов поиска

3. **Модуль агента** (`ai_agent.py`)
   - Реализация RAG Fusion
   - Обработка пользовательских запросов
   - Интеграция векторного поиска, BM25 и графа знаний
   
4. **API-сервер** (`api_server.py`)
   - REST API с FastAPI
   - Управление разговорами и историей
   - Загрузка файлов и обновление базы знаний

### Процесс обработки запроса

1. Генерация набора запросов на основе пользовательского вопроса
2. Поиск информации:
   - Поиск в графе знаний для структурированных данных
   - Гибридный поиск (векторный + BM25) для неструктурированных данных
   - Выполнение запросов и объединение результатов
3. Семантическое разбиение найденных документов
4. Выбор наиболее релевантных частей
5. Формирование контекста для модели и генерация ответа 